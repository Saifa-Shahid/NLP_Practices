{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import nltk\n",
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nt\n",
      "C:\\Users\\Saifa\\Desktop\\Project Chatbot\n"
     ]
    }
   ],
   "source": [
    "print(os.name)# This function gives the name of the operating system dependent module imported.                 The following names have currently been registered: ‘posix’, ‘nt’, ‘os2’, ‘ce’,                    ‘java’ and ‘riscos’\n",
    "print(os.getcwd())#Current Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Punkt Sentence Tokenizer\n",
    "\n",
    "#This tokenizer divides a text into a list of sentences, by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences. It must be trained on a large collection of plaintext in the target language before it can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Saifa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input data\n",
    "text = \"This is a sample text for Natural Language Processing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'sample',\n",
       " 'text',\n",
       " 'for',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_text = word_tokenize(text.lower())\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A RegEx, or Regular Expression, is a sequence of characters that forms a search pattern.\n",
    "\n",
    "#RegEx can be used to check if a string contains the specified search pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "punctuation = re.compile(r'[,./?!:()\\'\\\"|0-9]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'sample',\n",
       " 'text',\n",
       " 'for',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenized after removing punctuation\n",
    "\n",
    "post_punctuation = []\n",
    "for words in tokenized_text:\n",
    "  word = punctuation.sub(\"\", words)\n",
    "  if len(word) > 0:\n",
    "    post_punctuation.append(word)\n",
    "post_punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Saifa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')#pre-trained English [Part-of-Speech (POS]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('sample', 'JJ'),\n",
       " ('text', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('natural', 'JJ'),\n",
       " ('language', 'NN'),\n",
       " ('processing', 'NN')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag=nltk.pos_tag(post_punctuation)\n",
    "tag[0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(Maxnet chunker)The maxent_ne_chunker contains two pre-trained English named entity chunkers trained on an ACE corpus ...The objective of the Automatic Content Extraction (ACE) Program was to develop extraction technology to support automatic processing of source language data (in the form of natural text and as text derived from ASR and OCR). Automatic processing, defined at that time, included classification, filtering, and selection based on the language content of the source data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Saifa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Saifa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')# word download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('S', [('this', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('text', 'NN'), ('for', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN')])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity=nltk.chunk.ne_chunk(tag)\n",
    "entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = \"\"\"PRESIDENT GEORGE W. BUSH'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION.\"\"\"\n",
    "\n",
    "sample_text = \"\"\"PRESIDENT GEORGE W. BUSH'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION\"\"\"\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            chunked.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stopwords from english language corpus in wordnet dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Saifa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')# kind of language dictionary\n",
    "stop_set = stopwords.words('english')\n",
    "stop_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized after removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample', 'text', 'natural', 'language', 'processing']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[]\n",
    "for words in post_punctuation:\n",
    "  if words not in stop_set:\n",
    "    post_stop= words\n",
    "    a.append(post_stop)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming is the process of producing morphological variants of a root/base word. Stemming programs are commonly referred to as stemming algorithms or stemmers. \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Algorithms lists_\n",
    "=>Porter’s Stemmer \n",
    "=>Lovins Stemmer\n",
    "=>Dawson Stemmer\n",
    "=>Krovetz Stemmer\n",
    "=>Xerox Stemmer \n",
    "=>N-Gram Stemmer \n",
    "=>Snowball Stemmer\n",
    "=>Lancaster Stemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "pst = PorterStemmer()\n",
    "sbst = SnowballStemmer('english')\n",
    "lst = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How:how\n",
      "Attendance:attend\n",
      "crime:crime\n",
      "privacy:privaci\n",
      "procedure:procedur\n",
      "out:out\n"
     ]
    }
   ],
   "source": [
    "words_to_stem = ['How', 'Attendance', 'crime', 'privacy','procedure','out']\n",
    "for words in words_to_stem:\n",
    "  print(words + \":\" + pst.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How:how\n",
      "Attendance:attend\n",
      "crime:crime\n",
      "privacy:privaci\n",
      "procedure:procedur\n",
      "out:out\n"
     ]
    }
   ],
   "source": [
    "words_to_stem = ['How', 'Attendance', 'crime', 'privacy','procedure','out']\n",
    "for words in words_to_stem:\n",
    "  print(words + \":\" + sbst.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How:how\n",
      "Attendance:attend\n",
      "crime:crim\n",
      "privacy:priv\n",
      "procedure:proc\n",
      "out:out\n"
     ]
    }
   ],
   "source": [
    "words_to_stem = ['How', 'Attendance', 'crime', 'privacy','procedure','out']\n",
    "for words in words_to_stem:\n",
    "  print(words + \":\" + lst.stem(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Saifa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import wordnet       \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lnt = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How:How\n",
      "Attendance:Attendance\n",
      "crime:crime\n",
      "privacy:privacy\n",
      "procedure:procedure\n",
      "out:out\n"
     ]
    }
   ],
   "source": [
    "words_to_lematize = ['How', 'Attendance', 'crime', 'privacy','procedure','out']\n",
    "for words in words_to_lematize:\n",
    "  print(words + \":\" + lnt.lemmatize(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_lemmas(text):\n",
    "    for token in text:\n",
    "        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How          ADV    16331095434822636218   how\n",
      "to           PART   3791531372978436496    to\n",
      "give         VERB   11640825575873464194   give\n",
      "attendance   NOUN   987160471729325602     attendance\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"How to give attendance\")\n",
    "\n",
    "show_lemmas(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word similarity by calculating vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter two space-separated words\n",
      "health wealth\n",
      "health True 6.9867296 False\n",
      "wealth True 6.290415 False\n",
      "Similarity: 0.39325696\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "  \n",
    "nlp = spacy.load('en_core_web_md') \n",
    "  \n",
    "print(\"Enter two space-separated words\") \n",
    "words = input() \n",
    "  \n",
    "tokens = nlp(words) \n",
    "  \n",
    "for token in tokens: \n",
    "    # Printing the following attributes of each token. \n",
    "    # text: the word string, has_vector: if it contains \n",
    "    # a vector representation in the model,  \n",
    "    # vector_norm: the algebraic norm of the vector, \n",
    "    # is_oov: if the word is out of vocabulary. \n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov) \n",
    "  \n",
    "token1, token2 = tokens[0], tokens[1] \n",
    "  \n",
    "print(\"Similarity:\", token1.similarity(token2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "animals = \"dog cat hamster lion tiger elephant cheetah monkey gorilla antelope rabbit mouse rat zoo home pet fluffy wild domesticated\"\n",
    "animal_tokens = nlp(animals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_vectors = np.vstack([word.vector for word in animal_tokens if word.has_vector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_vecs_transformed = pca.fit_transform(animal_vectors)\n",
    "animal_vecs_transformed_1 = np.c_[animals.split(), animal_vecs_transformed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6.3609667 ,  4.5397177 ],\n",
       "       [-2.7626846 ,  0.40029016],\n",
       "       [-2.4463558 ,  8.294643  ],\n",
       "       [-0.97224855,  2.6122806 ],\n",
       "       [-8.803232  ,  3.2042487 ],\n",
       "       [-3.1677213 ,  0.5169825 ],\n",
       "       [ 4.081735  ,  0.80729216],\n",
       "       [ 0.89239943,  2.490714  ],\n",
       "       [-0.88437027, -3.2071075 ],\n",
       "       [-0.7535257 , -2.3989348 ],\n",
       "       [-4.2752867 , -0.95160556],\n",
       "       [-3.0975356 , -1.4161841 ],\n",
       "       [-4.577133  , -5.5937214 ],\n",
       "       [ 0.40506443, -7.2117863 ],\n",
       "       [ 1.5930843 , -6.0269513 ],\n",
       "       [ 3.0535555 ,  6.7535677 ],\n",
       "       [ 5.13563   , -5.367153  ],\n",
       "       [ 8.342184  , -3.2974172 ],\n",
       "       [14.59741   ,  5.851129  ]], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animal_vecs_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['dog', '-6.3609667', '4.5397177'],\n",
       "       ['cat', '-2.7626846', '0.40029016'],\n",
       "       ['hamster', '-2.4463558', '8.294643'],\n",
       "       ['lion', '-0.97224855', '2.6122806'],\n",
       "       ['tiger', '-8.803232', '3.2042487'],\n",
       "       ['elephant', '-3.1677213', '0.5169825'],\n",
       "       ['cheetah', '4.081735', '0.80729216'],\n",
       "       ['monkey', '0.89239943', '2.490714'],\n",
       "       ['gorilla', '-0.88437027', '-3.2071075'],\n",
       "       ['antelope', '-0.7535257', '-2.3989348'],\n",
       "       ['rabbit', '-4.2752867', '-0.95160556'],\n",
       "       ['mouse', '-3.0975356', '-1.4161841'],\n",
       "       ['rat', '-4.577133', '-5.5937214'],\n",
       "       ['zoo', '0.40506443', '-7.2117863'],\n",
       "       ['home', '1.5930843', '-6.0269513'],\n",
       "       ['pet', '3.0535555', '6.7535677'],\n",
       "       ['fluffy', '5.13563', '-5.367153'],\n",
       "       ['wild', '8.342184', '-3.2974172'],\n",
       "       ['domesticated', '14.59741', '5.851129']], dtype='<U32')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animal_vecs_transformed_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
